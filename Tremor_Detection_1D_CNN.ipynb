{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrfRccjLg8t4h+P6q/9Grn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "Y9f0AHEQsBcs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzRlmhZrmsC4",
        "outputId": "2bf9e768-8abf-4106-c17a-8764f51de084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KJx9-cy2u6Dv3KyotrhhXGRQ0vCWjo5Q\n",
            "To: /content/tremor_data.npz\n",
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 101MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['features', 'labels']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# get tremor data (.npz file) from drive\n",
        "# link: https://drive.google.com/file/d/1KJx9-cy2u6Dv3KyotrhhXGRQ0vCWjo5Q/view?usp=drive_link\n",
        "!pip install gdown\n",
        "import gdown\n",
        "\n",
        "file_id = \"1KJx9-cy2u6Dv3KyotrhhXGRQ0vCWjo5Q\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"tremor_data.npz\", quiet=False)\n",
        "\n",
        "data = np.load('tremor_data.npz')\n",
        "print(data.files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is comprised of **feature arrays** and **label arrays** which have already been processed. Features are in 3 second segments, sampled at 100Hz so 300 data points, and across 3 dimensional channels, x y z."
      ],
      "metadata": {
        "id": "lT0VNh1Qo4tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Feature array shape: {data['features'].shape}\")\n",
        "print(f\"Label array shape: {data['labels'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfH0RG2RoBz7",
        "outputId": "1e3d65cc-d6ff-4938-919d-6c19c39c5a13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature array shape: (3091, 300, 3)\n",
            "Label array shape: (3091,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = np.unique(data['labels'], return_counts=True)\n",
        "class_name = {0: 'non-tremor', 1: 'pre-tremor', 2: 'tremor'}\n",
        "label_counts = {class_name[int(lbl)]: int(count) for lbl, count in zip(counts[0], counts[1])}\n",
        "print(f\"Label Distribution: {label_counts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ivwfjvUpvMq",
        "outputId": "31f8df71-426a-4d4d-a3ad-2460b573de33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution: {'non-tremor': 2997, 'pre-tremor': 28, 'tremor': 66}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are unexpectly a serious **class imbalance issue**, where non-tremor (label 0) has a greater presence than pre-tremor (label 1) and tremor (label 2). Something we can do before even training is **downsample the dominant class**, 'non-tremor'."
      ],
      "metadata": {
        "id": "2KriOPjZvhuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(signal, labels, percent_tremor=10):\n",
        "    '''\n",
        "    percent_tremor: Percent of samples that are labeled either 'pre-tremor' (1) or 'tremor' (2) in the downsampled dataset.\n",
        "    '''\n",
        "    count = np.unique(labels, return_counts=True)[1]\n",
        "    num_tremor = count[1] + count[2]\n",
        "    # adjust non tremor sample count\n",
        "    num_non_tremor = int((num_tremor / percent_tremor) * 100) - num_tremor\n",
        "    print(f'Downsampling to {num_non_tremor} of non-tremor samples from {count[0]}')\n",
        "    print(f'Downsampled set has {num_tremor} amount of positive tremor labels.')\n",
        "\n",
        "    if len(labels) - num_tremor < num_non_tremor:\n",
        "        print(f\"Chosen percent_tremor: {percent_tremor} is upsampling 'non_tremor' cases.\")\n",
        "        return signal, labels\n",
        "    else:\n",
        "        # get indicies of each sample with respective label\n",
        "        zero_mask = labels == 0\n",
        "        tremor_mask = labels !=0\n",
        "\n",
        "        # randomly select 'non_tremor' samples to keep\n",
        "        kept_zeroes = np.random.choice(np.where(zero_mask)[0], num_non_tremor, replace=False)\n",
        "        kept_sample_idx = np.concatenate((kept_zeroes, np.where(tremor_mask)[0]))\n",
        "        shuffled_idx = np.random.permutation(kept_sample_idx)\n",
        "\n",
        "        return signal[shuffled_idx], labels[shuffled_idx]"
      ],
      "metadata": {
        "id": "qauIfiisw2Ai"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downsampled_signal, downsampled_labels = downsample(data['features'], data['labels'], percent_tremor=10)\n",
        "print(f\"Downsampled Feature array shape: {downsampled_signal.shape}\")\n",
        "print(f\"Downsampled Label array shape: {downsampled_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m2YS7ei3m8M",
        "outputId": "87ec5001-f072-4dac-97ea-61dd1f070128"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downsampling to 846 of non-tremor samples from 2997\n",
            "Downsampled set has 94 amount of positive tremor labels.\n",
            "Downsampled Feature array shape: (940, 300, 3)\n",
            "Downsampled Label array shape: (940,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch dataset for tremor data."
      ],
      "metadata": {
        "id": "IG8WxmgTsmr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TremorDataset(Dataset):\n",
        "    def __init__(self, signal, labels):\n",
        "        '''\n",
        "        signal (ndarray): 3D array of shape (num_samples, num_points, num_channels)\n",
        "        labels (ndarray): 1D array of shape (num_samples,)\n",
        "        '''\n",
        "        # change signal dimension to (num_samples, num_channels, num_points) since 1D Conv passes over lowest dimension\n",
        "        self.signal = torch.tensor(signal, dtype=torch.float32).permute(0, 2, 1)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.signal[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "Aoyeimlhrdtx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TremorDataset(downsampled_signal, downsampled_labels)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(20))\n",
        "\n",
        "# create dataloaders, only using train and val for proof of concept\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# one batch\n",
        "signal_batch, label_batch = next(iter(train_loader))\n",
        "print(signal_batch.shape) # expecting (B, channels, sequence) -> (B, 3, 300)\n",
        "print(label_batch.shape) # expecting (B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1b1Zvf33aFY",
        "outputId": "cc9431c3-2745-485c-de6c-ded37afa3282"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 300])\n",
            "torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: 1D CNN**"
      ],
      "metadata": {
        "id": "uz96hzaS82Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''GPT generated code'''\n",
        "class TremorCNN(nn.Module):\n",
        "    def __init__(self, num_channels=3, num_classes=3):\n",
        "        super(TremorCNN, self).__init__()\n",
        "\n",
        "        # Conv1: in_channels=3 (x,y,z), out_channels=16, kernel_size=5\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=16, kernel_size=5)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "\n",
        "        # Conv2: 16 -> 32\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Adaptive pooling to fix output length\n",
        "        self.pool = nn.AdaptiveMaxPool1d(50)  # output length = 50\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 50, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # num_classes = 3 (0,1,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, 3, sequence_length)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))   # -> (batch, 16, L1)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))   # -> (batch, 32, L2)\n",
        "        x = self.pool(x)                       # -> (batch, 32, 50)\n",
        "        x = x.view(x.size(0), -1)             # flatten -> (batch, 32*50)\n",
        "        x = F.relu(self.fc1(x))               # -> (batch, 64)\n",
        "        x = self.fc2(x)                       # -> (batch, num_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ejl_xfUp8-P3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**"
      ],
      "metadata": {
        "id": "X2i0-UeUcUqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label, count = np.unique(downsampled_labels, return_counts=True)\n",
        "class_weights = torch.tensor(1.0 / count, dtype=torch.float32)\n",
        "class_weights /= class_weights.sum() # normalize so training loss is easier to read (not scaled ny 1e-3 or so)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, class_weights, epochs=5, lr=1e-2):\n",
        "    # using weighted loss to mitigate effects of having less tremor labels than 'non-tremor'\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_pred = []\n",
        "        all_labels = []\n",
        "\n",
        "        for signals, labels in train_loader:\n",
        "            signals, labels = signals.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(signals)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batch_pred = torch.argmax(outputs, dim=1)\n",
        "            # each batch, get associated predictions and labels to compute confusion matrix metrics per epoch\n",
        "            epoch_pred.extend(batch_pred.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        epoch_loss = running_loss / len(labels)\n",
        "        # these metrics better than accuracy for class imbalance\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, epoch_pred, average='weighted')\n",
        "\n",
        "        print(f'Epoch {epoch}/{epochs} | Train Loss: {epoch_loss:.4f} | Train Precision: {precision:.4f} | Train Recall: {recall:.4f} | F1: {f1:.4f}')\n",
        "\n",
        "        #-----------\n",
        "        # VALIDATION\n",
        "        #-----------\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        with torch.no_grad():\n",
        "            for signals, labels in val_loader:\n",
        "                signals, labels = signals.to(device), labels.to(device)\n",
        "                outputs = model(signals)\n",
        "                batch_pred = torch.argmax(outputs, dim=1)\n",
        "                val_preds.extend(batch_pred.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
        "        print(f'Epoch {epoch}/{epochs} | Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f}\\n')"
      ],
      "metadata": {
        "id": "mzy9NgaXcUJ-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(TremorCNN(), train_loader, val_loader, class_weights, epochs=20, lr=1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYsTmtlyQnrl",
        "outputId": "26e3a98a-fb93-4731-960e-372330759736"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 5.5277 | Train Precision: 0.8935 | Train Recall: 0.7846 | F1: 0.8278\n",
            "Epoch 1/20 | Val Precision: 0.8907 | Val Recall: 0.8617 | Val F1: 0.8675\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 2.1571 | Train Precision: 0.8895 | Train Recall: 0.8457 | F1: 0.8599\n",
            "Epoch 2/20 | Val Precision: 0.8931 | Val Recall: 0.8777 | Val F1: 0.8783\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 2.1339 | Train Precision: 0.9007 | Train Recall: 0.8404 | F1: 0.8615\n",
            "Epoch 3/20 | Val Precision: 0.8885 | Val Recall: 0.8457 | Val F1: 0.8567\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 1.9517 | Train Precision: 0.9006 | Train Recall: 0.8444 | F1: 0.8616\n",
            "Epoch 4/20 | Val Precision: 0.9026 | Val Recall: 0.7660 | Val F1: 0.8142\n",
            "\n",
            "Epoch 5/20 | Train Loss: 1.7847 | Train Precision: 0.9179 | Train Recall: 0.8165 | F1: 0.8546\n",
            "Epoch 5/20 | Val Precision: 0.8666 | Val Recall: 0.7872 | Val F1: 0.8202\n",
            "\n",
            "Epoch 6/20 | Train Loss: 1.7955 | Train Precision: 0.9141 | Train Recall: 0.8298 | F1: 0.8623\n",
            "Epoch 6/20 | Val Precision: 0.8857 | Val Recall: 0.8191 | Val F1: 0.8479\n",
            "\n",
            "Epoch 7/20 | Train Loss: 1.8123 | Train Precision: 0.9173 | Train Recall: 0.8085 | F1: 0.8505\n",
            "Epoch 7/20 | Val Precision: 0.8966 | Val Recall: 0.7766 | Val F1: 0.8251\n",
            "\n",
            "Epoch 8/20 | Train Loss: 1.4260 | Train Precision: 0.9233 | Train Recall: 0.8431 | F1: 0.8725\n",
            "Epoch 8/20 | Val Precision: 0.9033 | Val Recall: 0.7819 | Val F1: 0.8317\n",
            "\n",
            "Epoch 9/20 | Train Loss: 1.5430 | Train Precision: 0.9298 | Train Recall: 0.8617 | F1: 0.8877\n",
            "Epoch 9/20 | Val Precision: 0.8903 | Val Recall: 0.7819 | Val F1: 0.8277\n",
            "\n",
            "Epoch 10/20 | Train Loss: 1.2223 | Train Precision: 0.9370 | Train Recall: 0.8910 | F1: 0.9081\n",
            "Epoch 10/20 | Val Precision: 0.8935 | Val Recall: 0.6809 | Val F1: 0.7638\n",
            "\n",
            "Epoch 11/20 | Train Loss: 1.3305 | Train Precision: 0.9370 | Train Recall: 0.8511 | F1: 0.8827\n",
            "Epoch 11/20 | Val Precision: 0.9051 | Val Recall: 0.8511 | Val F1: 0.8725\n",
            "\n",
            "Epoch 12/20 | Train Loss: 1.0480 | Train Precision: 0.9368 | Train Recall: 0.8697 | F1: 0.8927\n",
            "Epoch 12/20 | Val Precision: 0.8978 | Val Recall: 0.8351 | Val F1: 0.8612\n",
            "\n",
            "Epoch 13/20 | Train Loss: 0.9548 | Train Precision: 0.9423 | Train Recall: 0.8856 | F1: 0.9053\n",
            "Epoch 13/20 | Val Precision: 0.8743 | Val Recall: 0.8032 | Val F1: 0.8353\n",
            "\n",
            "Epoch 14/20 | Train Loss: 0.9036 | Train Precision: 0.9498 | Train Recall: 0.8963 | F1: 0.9158\n",
            "Epoch 14/20 | Val Precision: 0.8890 | Val Recall: 0.7766 | Val F1: 0.8259\n",
            "\n",
            "Epoch 15/20 | Train Loss: 1.0165 | Train Precision: 0.9348 | Train Recall: 0.8551 | F1: 0.8829\n",
            "Epoch 15/20 | Val Precision: 0.8973 | Val Recall: 0.7340 | Val F1: 0.8008\n",
            "\n",
            "Epoch 16/20 | Train Loss: 0.7766 | Train Precision: 0.9496 | Train Recall: 0.9016 | F1: 0.9185\n",
            "Epoch 16/20 | Val Precision: 0.8558 | Val Recall: 0.8351 | Val F1: 0.8450\n",
            "\n",
            "Epoch 17/20 | Train Loss: 0.7241 | Train Precision: 0.9472 | Train Recall: 0.9149 | F1: 0.9258\n",
            "Epoch 17/20 | Val Precision: 0.8629 | Val Recall: 0.8617 | Val F1: 0.8621\n",
            "\n",
            "Epoch 18/20 | Train Loss: 0.5473 | Train Precision: 0.9561 | Train Recall: 0.9375 | F1: 0.9433\n",
            "Epoch 18/20 | Val Precision: 0.8794 | Val Recall: 0.8085 | Val F1: 0.8395\n",
            "\n",
            "Epoch 19/20 | Train Loss: 0.4171 | Train Precision: 0.9594 | Train Recall: 0.9348 | F1: 0.9426\n",
            "Epoch 19/20 | Val Precision: 0.8769 | Val Recall: 0.7872 | Val F1: 0.8248\n",
            "\n",
            "Epoch 20/20 | Train Loss: 0.6647 | Train Precision: 0.9508 | Train Recall: 0.9122 | F1: 0.9239\n",
            "Epoch 20/20 | Val Precision: 0.8943 | Val Recall: 0.7340 | Val F1: 0.7957\n",
            "\n"
          ]
        }
      ]
    }
  ]
}